{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-10-08T21:00:28.330453Z","iopub.execute_input":"2022-10-08T21:00:28.330828Z","iopub.status.idle":"2022-10-08T21:00:28.348075Z","shell.execute_reply.started":"2022-10-08T21:00:28.330798Z","shell.execute_reply":"2022-10-08T21:00:28.347180Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/input/hate-speech-and-offensive-language-dataset/labeled_data.csv\n/kaggle/input/twitter-sentiment-analysis-hatred-speech/train.csv\n/kaggle/input/twitter-sentiment-analysis-hatred-speech/test.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"df_twitter=pd.read_csv(\"twitter-sentiment-analysis-hatred-speech/train.csv\")\ndf_twitter.drop('id',axis=1,inplace=True)\ndf_offensive=pd.read_csv(\"hate-speech-and-offensive-language-dataset/labeled_data.csv\")\ndf_offensive.drop(['Unnamed: 0','count','hate_speech','offensive_language','neither'],axis=1,inplace=True)\ndf_offensive[\"class\"].replace({0: 1}, inplace=True)\ndf_offensive[\"class\"].replace({2: 0}, inplace=True)\ndf_offensive.rename(columns ={'class':'label'}, inplace = True)\ndf_offensive.iloc[0]['tweet']\ndf_offensive.iloc[5]['tweet']\nframe=[df_twitter,df_offensive]\ndf = pd.concat(frame)\n","metadata":{"execution":{"iopub.status.busy":"2022-10-08T21:00:30.019193Z","iopub.execute_input":"2022-10-08T21:00:30.019580Z","iopub.status.idle":"2022-10-08T21:00:30.186267Z","shell.execute_reply.started":"2022-10-08T21:00:30.019544Z","shell.execute_reply":"2022-10-08T21:00:30.185476Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import re\nimport nltk\nstemmer = nltk.SnowballStemmer(\"english\")\nfrom nltk.corpus import stopwords\nimport string\nstopword=set(stopwords.words('english'))\n\ndef clean_text(text):\n    text = str(text).lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    text = [word for word in text.split(' ') if word not in stopword]\n    text=\" \".join(text)\n    text = [stemmer.stem(word) for word in text.split(' ')]\n    text=\" \".join(text)\n    return text\n\ndf['tweet']=df['tweet'].apply(clean_text)\n","metadata":{"execution":{"iopub.status.busy":"2022-10-08T21:00:32.908244Z","iopub.execute_input":"2022-10-08T21:00:32.908589Z","iopub.status.idle":"2022-10-08T21:00:42.082444Z","shell.execute_reply.started":"2022-10-08T21:00:32.908553Z","shell.execute_reply":"2022-10-08T21:00:42.081571Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"df['tweet']=df['tweet'].apply(clean_text)","metadata":{"execution":{"iopub.status.busy":"2022-10-08T21:00:42.084092Z","iopub.execute_input":"2022-10-08T21:00:42.084459Z","iopub.status.idle":"2022-10-08T21:00:49.458484Z","shell.execute_reply.started":"2022-10-08T21:00:42.084422Z","shell.execute_reply":"2022-10-08T21:00:49.457648Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport pandas as pd\n\nx=df['tweet']\ny=df['label']\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, random_state=42)\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ncount = CountVectorizer(stop_words='english', ngram_range=(1,5))\nx_train_vectorizer=count.fit_transform(x_train)\nx_test_vectorizer=count.transform(x_test)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-10-08T21:00:49.460232Z","iopub.execute_input":"2022-10-08T21:00:49.460606Z","iopub.status.idle":"2022-10-08T21:00:54.563697Z","shell.execute_reply.started":"2022-10-08T21:00:49.460571Z","shell.execute_reply":"2022-10-08T21:00:54.556568Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"42558 42558\n14187 14187\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"array([[0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       ...,\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0]])"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfTransformer\ntfidf = TfidfTransformer()\n\nx_train_tfidf = tfidf.fit_transform(x_train_vectorizer)\n\nx_train_tfidf.toarray()\nx_test_tfidf = tfidf.transform(x_test_vectorizer)\n\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n\nmodel_vectorizer= MultinomialNB().fit(x_train_vectorizer, y_train)\nprediction_vectorizer=model_vectorizer.predict(x_test_vectorizer)\n\nmodel_tfidf= MultinomialNB().fit(x_train_tfidf, y_train)\nprediction_tfidf=model_tfidf.predict(x_test_tfidf)","metadata":{"execution":{"iopub.status.busy":"2022-10-08T21:00:54.566379Z","iopub.execute_input":"2022-10-08T21:00:54.566973Z","iopub.status.idle":"2022-10-08T21:00:57.526220Z","shell.execute_reply.started":"2022-10-08T21:00:54.566910Z","shell.execute_reply":"2022-10-08T21:00:57.525197Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"[[7883  570]\n [ 457 5277]]\n              precision    recall  f1-score   support\n\n           0       0.95      0.93      0.94      8453\n           1       0.90      0.92      0.91      5734\n\n    accuracy                           0.93     14187\n   macro avg       0.92      0.93      0.93     14187\nweighted avg       0.93      0.93      0.93     14187\n\n              precision    recall  f1-score   support\n\n           0       0.91      0.97      0.94      8453\n           1       0.95      0.85      0.90      5734\n\n    accuracy                           0.92     14187\n   macro avg       0.93      0.91      0.92     14187\nweighted avg       0.92      0.92      0.92     14187\n\n[[8213  240]\n [ 857 4877]]\n","output_type":"stream"}]},{"cell_type":"code","source":"import xgboost as xgb\nxgb_model=xgb.XGBClassifier(\n        learning_rate=0.1,\n        max_depth=7,\n        n_estimators=80,\n        use_label_encoder=False,\n        eval_metric='auc' )\n\nxgb_model_vectorizer = xgb_model.fit(x_train_vectorizer, y_train)\nxgb_predictions_vectorizer=xgb_model_vectorizer.predict(x_test_vectorizer)\n\nxgb_model = xgb_model.fit(x_train_tfidf, y_train)\nxgb_predictions=xgb_model.predict(x_test_tfidf)","metadata":{"execution":{"iopub.status.busy":"2022-10-08T21:00:57.527620Z","iopub.execute_input":"2022-10-08T21:00:57.527960Z","iopub.status.idle":"2022-10-08T21:02:34.465679Z","shell.execute_reply.started":"2022-10-08T21:00:57.527924Z","shell.execute_reply":"2022-10-08T21:02:34.464060Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"[[8368   85]\n [ 925 4809]]\n              precision    recall  f1-score   support\n\n           0       0.90      0.99      0.94      8453\n           1       0.98      0.84      0.90      5734\n\n    accuracy                           0.93     14187\n   macro avg       0.94      0.91      0.92     14187\nweighted avg       0.93      0.93      0.93     14187\n\n[[8362   91]\n [ 935 4799]]\n              precision    recall  f1-score   support\n\n           0       0.90      0.99      0.94      8453\n           1       0.98      0.84      0.90      5734\n\n    accuracy                           0.93     14187\n   macro avg       0.94      0.91      0.92     14187\nweighted avg       0.93      0.93      0.93     14187\n\n","output_type":"stream"}]},{"cell_type":"code","source":"from keras.models import Model\nfrom keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding,SpatialDropout1D\nfrom keras.optimizers import RMSprop\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing import sequence\nfrom keras.utils import to_categorical\nfrom keras.callbacks import EarlyStopping\nfrom keras.models import Sequential\n\nmax_words = 50000\nmax_len = 300\ntokenizer = Tokenizer(num_words=max_words)\ntokenizer.fit_on_texts(x_train)\nsequences = tokenizer.texts_to_sequences(x_train)\nsequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)\n\nmodel = Sequential()\nmodel.add(Embedding(max_words, 100, input_length=max_len))\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.summary()\nmodel.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2022-10-08T21:02:34.467121Z","iopub.execute_input":"2022-10-08T21:02:34.467461Z","iopub.status.idle":"2022-10-08T21:02:41.226224Z","shell.execute_reply.started":"2022-10-08T21:02:34.467426Z","shell.execute_reply":"2022-10-08T21:02:41.225354Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        (None, 300, 100)          5000000   \n_________________________________________________________________\nspatial_dropout1d (SpatialDr (None, 300, 100)          0         \n_________________________________________________________________\nlstm (LSTM)                  (None, 100)               80400     \n_________________________________________________________________\ndense (Dense)                (None, 1)                 101       \n=================================================================\nTotal params: 5,080,501\nTrainable params: 5,080,501\nNon-trainable params: 0\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"from keras.callbacks import EarlyStopping,ModelCheckpoint\n\nstop = EarlyStopping(\n    monitor='val_accuracy', \n    mode='max',\n    patience=5\n)\n\ncheckpoint= ModelCheckpoint(\n    filepath='./',\n    save_weights_only=True,\n    monitor='val_accuracy',\n    mode='max',\n    save_best_only=True)\n\n\nhistory=model.fit(sequences_matrix,y_train,batch_size=1024,epochs=10,\n          validation_split=0.2,callbacks=[stop,checkpoint])\ntest_sequences = tokenizer.texts_to_sequences(x_test)\ntest_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)\naccr = model.evaluate(test_sequences_matrix,y_test)\nlstm_prediction=model.predict(test_sequences_matrix)\nres=[]\nfor prediction in lstm_prediction:\n    if prediction[0]<0.5:\n        res.append(0)\n    else:\n        res.append(1)\nprint(confusion_matrix(y_test,res))\n","metadata":{"execution":{"iopub.status.busy":"2022-10-08T21:02:41.228330Z","iopub.execute_input":"2022-10-08T21:02:41.228928Z","iopub.status.idle":"2022-10-08T21:10:05.230015Z","shell.execute_reply.started":"2022-10-08T21:02:41.228887Z","shell.execute_reply":"2022-10-08T21:10:05.228549Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Epoch 1/10\n34/34 [==============================] - 43s 1s/step - loss: 0.6679 - accuracy: 0.6597 - val_loss: 0.3366 - val_accuracy: 0.9079\nEpoch 2/10\n34/34 [==============================] - 38s 1s/step - loss: 0.2946 - accuracy: 0.9145 - val_loss: 0.2211 - val_accuracy: 0.9252\nEpoch 3/10\n34/34 [==============================] - 39s 1s/step - loss: 0.1807 - accuracy: 0.9397 - val_loss: 0.1703 - val_accuracy: 0.9388\nEpoch 4/10\n34/34 [==============================] - 39s 1s/step - loss: 0.1214 - accuracy: 0.9584 - val_loss: 0.1657 - val_accuracy: 0.9400\nEpoch 5/10\n34/34 [==============================] - 39s 1s/step - loss: 0.0975 - accuracy: 0.9667 - val_loss: 0.1627 - val_accuracy: 0.9414\nEpoch 6/10\n34/34 [==============================] - 39s 1s/step - loss: 0.0780 - accuracy: 0.9733 - val_loss: 0.1684 - val_accuracy: 0.9398\nEpoch 7/10\n34/34 [==============================] - 40s 1s/step - loss: 0.0711 - accuracy: 0.9768 - val_loss: 0.1692 - val_accuracy: 0.9409\nEpoch 8/10\n34/34 [==============================] - 38s 1s/step - loss: 0.0597 - accuracy: 0.9801 - val_loss: 0.1760 - val_accuracy: 0.9381\nEpoch 9/10\n34/34 [==============================] - 40s 1s/step - loss: 0.0518 - accuracy: 0.9845 - val_loss: 0.1773 - val_accuracy: 0.9367\nEpoch 10/10\n34/34 [==============================] - 39s 1s/step - loss: 0.0476 - accuracy: 0.9853 - val_loss: 0.1862 - val_accuracy: 0.9401\n444/444 [==============================] - 25s 56ms/step - loss: 0.2100 - accuracy: 0.9328\n[[8029  424]\n [ 530 5204]]\n","output_type":"stream"}]},{"cell_type":"code","source":"import pickle\nwith open('tokenizer.pickle', 'wb') as handle:\n    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\nmodel.save(\"hate&abusive_model.h5\")","metadata":{"execution":{"iopub.status.busy":"2022-10-08T21:10:05.232031Z","iopub.execute_input":"2022-10-08T21:10:05.232285Z","iopub.status.idle":"2022-10-08T21:10:05.399952Z","shell.execute_reply.started":"2022-10-08T21:10:05.232258Z","shell.execute_reply":"2022-10-08T21:10:05.399057Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"load_model=keras.models.load_model(\"./hate&abusive_model.h5\")\nwith open('tokenizer.pickle', 'rb') as handle:\n    load_tokenizer = pickle.load(handle)","metadata":{"execution":{"iopub.status.busy":"2022-10-08T21:10:05.437233Z","iopub.status.idle":"2022-10-08T21:10:05.438040Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = 'I hate my country'\ndef clean_text(text):\n    print(text)\n    text = str(text).lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    print(text)\n    text = [word for word in text.split(' ') if word not in stopword]\n    text=\" \".join(text)\n    text = [stemmer.stem(word) for word in text.split(' ')]\n    text=\" \".join(text)\n    return text\ntest=[clean_text(test)]\nprint(test)\nseq = load_tokenizer.texts_to_sequences(test)\npadded = sequence.pad_sequences(seq, maxlen=300)\nprint(seq)\npred = load_model.predict(padded)\nprint(\"pred\", pred)\nif pred<0.5:\n    print(\"no hate\")\nelse:\n    print(\"hate and abusive\")","metadata":{"execution":{"iopub.status.busy":"2022-10-08T21:10:05.401418Z","iopub.execute_input":"2022-10-08T21:10:05.401793Z","iopub.status.idle":"2022-10-08T21:10:05.434032Z","shell.execute_reply.started":"2022-10-08T21:10:05.401757Z","shell.execute_reply":"2022-10-08T21:10:05.432699Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"I hate my country\ni hate my country\n['hate countri']\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-eacebce39bbb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclean_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mseq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mpadded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'load_tokenizer' is not defined"],"ename":"NameError","evalue":"name 'load_tokenizer' is not defined","output_type":"error"}]},{"cell_type":"code","source":"test = 'nice job!'\ndef clean_text(text):\n    print(text)\n    text = str(text).lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    print(text)\n    text = [word for word in text.split(' ') if word not in stopword]\n    text=\" \".join(text)\n    text = [stemmer.stem(word) for word in text.split(' ')]\n    text=\" \".join(text)\n    return text\ntest=[clean_text(test)]\nprint(test)\nseq = load_tokenizer.texts_to_sequences(test)\npadded = sequence.pad_sequences(seq, maxlen=300)\nprint(seq)\npred = load_model.predict(padded)\nprint(\"pred\", pred)\nif pred<0.5:\n    print(\"no hate\")\nelse:\n    print(\"hate and abusive\")","metadata":{"execution":{"iopub.status.busy":"2022-10-08T21:10:05.435167Z","iopub.status.idle":"2022-10-08T21:10:05.435972Z"},"trusted":true},"execution_count":null,"outputs":[]}]}